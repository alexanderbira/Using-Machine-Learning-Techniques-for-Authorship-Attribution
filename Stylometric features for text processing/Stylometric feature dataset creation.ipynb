{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c8548c-87b0-496c-b89e-2fae71f0bc72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T17:45:40.904167Z",
     "iopub.status.busy": "2022-08-20T17:45:40.903408Z",
     "iopub.status.idle": "2022-08-20T17:46:35.622054Z",
     "shell.execute_reply": "2022-08-20T17:46:35.620540Z",
     "shell.execute_reply.started": "2022-08-20T17:45:40.903958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install stylometrix and dependencies\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_trf\n",
    "!python -m pip install stylo_metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76777e7e-541e-4bb3-a6df-1e93361c90b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T17:46:43.205640Z",
     "iopub.status.busy": "2022-08-20T17:46:43.205294Z",
     "iopub.status.idle": "2022-08-20T17:46:55.244673Z",
     "shell.execute_reply": "2022-08-20T17:46:55.243361Z",
     "shell.execute_reply.started": "2022-08-20T17:46:43.205612Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import stylo_metrix\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "nlp = spacy.load('en_core_web_trf')  # for English\n",
    "nlp.add_pipe(\"stylo_metrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0781a64d-ee3f-4bf8-b54c-94ae17524259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:38:50.799002Z",
     "iopub.status.busy": "2022-08-20T18:38:50.798371Z",
     "iopub.status.idle": "2022-08-20T18:38:50.806519Z",
     "shell.execute_reply": "2022-08-20T18:38:50.805236Z",
     "shell.execute_reply.started": "2022-08-20T18:38:50.799001Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_text_info(text):\n",
    "    # only keep the values\n",
    "    return list(map(lambda x: x[\"value\"], list(nlp(text)._.stylo_metrix_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb61735-06f0-433f-99d5-eaa570f0e829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:44:07.493656Z",
     "iopub.status.busy": "2022-08-20T18:44:07.493153Z",
     "iopub.status.idle": "2022-08-20T18:44:25.644251Z",
     "shell.execute_reply": "2022-08-20T18:44:25.642349Z",
     "shell.execute_reply.started": "2022-08-20T18:44:07.493626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "\n",
    "# get all the files\n",
    "path_main = 'data/'\n",
    "n = 1000  # chunk length\n",
    "\n",
    "works_files = {\n",
    "    \"austen\": [],\n",
    "    # \"carroll\": [],\n",
    "    # \"dickens\": [],\n",
    "    # \"kipling\": [],\n",
    "    # \"poe\": [],\n",
    "    \"shakespeare\": [],\n",
    "    \"twain\": [],\n",
    "    # \"verne\": [],\n",
    "    \"wells\": []\n",
    "}\n",
    "\n",
    "for author in works_files:\n",
    "    path = path_main + author\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            works_files[author].append(os.path.join(root, file))\n",
    "\n",
    "# convert the files to 1000-character chunks\n",
    "works_chunks = {\n",
    "    \"austen\": [],\n",
    "    # \"carroll\": [],\n",
    "    # \"dickens\": [],\n",
    "    # \"kipling\": [],\n",
    "    # \"poe\": [],\n",
    "    \"shakespeare\": [],\n",
    "    \"twain\": [],\n",
    "    # \"verne\": [],\n",
    "    \"wells\": []\n",
    "}\n",
    "\n",
    "for author in works_chunks:\n",
    "    for file in works_files[author]:\n",
    "        if \".DS_Store\" in file:\n",
    "            continue\n",
    "        with open(file, \"r\") as open_file:\n",
    "            # there are lots of unnecessary line breaks\n",
    "            text = open_file.read().replace(\"\\n\", \" \")\n",
    "            sentences = text.split(\".\")\n",
    "            newchunks = []\n",
    "            newchunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                newchunk += sentence + \".\"  # removed during splitting\n",
    "                if len(newchunk) >= n:\n",
    "                    newchunks.append(newchunk)\n",
    "                    newchunk = \"\"\n",
    "\n",
    "            works_chunks[author] += newchunks\n",
    "\n",
    "altmax = round(4000000/n)\n",
    "numchunks = min(len(works_chunks[\"austen\"]), altmax)+min(len(works_chunks[\"shakespeare\"]),\n",
    "                                                         altmax)+min(len(works_chunks[\"twain\"]), altmax)+min(len(works_chunks[\"wells\"]), altmax)\n",
    "\n",
    "\n",
    "# convert the chunks to stylometric lists\n",
    "works_features = {\n",
    "    \"austen\": [],\n",
    "    # \"carroll\": [],\n",
    "    # \"dickens\": [],\n",
    "    # \"kipling\": [],\n",
    "    # \"poe\": [],\n",
    "    \"shakespeare\": [],\n",
    "    \"twain\": [],\n",
    "    # \"verne\": [],\n",
    "    \"wells\": []\n",
    "}\n",
    "\n",
    "i = 0\n",
    "start_time = time.time()\n",
    "for author in works_features:\n",
    "    print(author)\n",
    "    chunknum = 0\n",
    "    for chunk in works_chunks[author]:\n",
    "        try:\n",
    "            works_features[author].append(get_text_info(chunk))\n",
    "\n",
    "            chunknum += 1\n",
    "            if chunknum == altmax:\n",
    "                break\n",
    "            i += 1\n",
    "            if i % 10 == 0:\n",
    "                print(\n",
    "                    f\"{i} / {numchunks}, ~{round((((time.time()-start_time)/i)*(numchunks-i))/60)} minutes left\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error at {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd10a89-4c4c-42fb-b609-7a6210f575d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing data\n",
    "\n",
    "# 1 = only train, 0 = only test\n",
    "train_test_ratio = 0.5\n",
    "\n",
    "#[train, test]\n",
    "split_data = [{}, {}]\n",
    "\n",
    "# randomise data\n",
    "for author in works_features:\n",
    "    print(author)\n",
    "\n",
    "    random.shuffle(works_features[author])\n",
    "    data_length = len(works_features[author])\n",
    "    split_data[0][author] = works_features[author][0:round(data_length*train_test_ratio)]\n",
    "    split_data[1][author] = works_features[author][round(\n",
    "        data_length*train_test_ratio):data_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb9e1e-b586-4f03-81d9-a642ceb58b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw data format to rectangular data format and save to file\n",
    "\n",
    "path_main = ''\n",
    "\n",
    "# training data\n",
    "new_format = []\n",
    "author_index = 0\n",
    "for author in split_data[0]:\n",
    "    for entry in split_data[0][author]:\n",
    "        new_format.append(entry + [author_index])\n",
    "    author_index += 1\n",
    "random.shuffle(new_format)\n",
    "with open(path_main+\"train_rectangle.json\", \"w+\") as test_file:\n",
    "    json.dump(new_format, test_file)\n",
    "\n",
    "\n",
    "# testing data\n",
    "new_format = []\n",
    "author_index = 0\n",
    "for author in split_data[1]:\n",
    "    for entry in split_data[1][author]:\n",
    "        new_format.append(entry + [author_index])\n",
    "    author_index += 1\n",
    "random.shuffle(new_format)\n",
    "with open(path_main+\"test_rectangle.json\", \"w+\") as test_file:\n",
    "    json.dump(new_format, test_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
